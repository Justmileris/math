{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from chardet import detect\n",
    "from IPython.display import display\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn import model_selection\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath = 'data/train.csv'\n",
    "test_filepath = 'data/test.csv'\n",
    "\n",
    "def get_encoding(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']\n",
    "\n",
    "data_train = pd.read_csv(train_filepath, sep=\",\", encoding = get_encoding(train_filepath))\n",
    "data_test = pd.read_csv(test_filepath, sep=\",\", encoding = get_encoding(test_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explore + clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = data_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to numeric\n",
    "\n",
    "print(df_temp['B'].unique())\n",
    "print(len(df_temp['B'].unique()))\n",
    "df_temp['B_numeric'] = df_temp['B'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.hist(bins=30, figsize=(12,12))\n",
    "pl.suptitle(\"Histogram for each numeric input variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.dropna()\n",
    "\n",
    "feature_names = ['A', 'NN', 'B_numeric']\n",
    "X = df_temp[feature_names]\n",
    "y = df_temp['P']\n",
    "cmap = cm.get_cmap('gnuplot')\n",
    "scatter = scatter_matrix(X, c = y, s=40, figsize=(12,12))\n",
    "plt.suptitle('Scatter-matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NN column to range classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['NN'].plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), title='NNs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(data_train['NN']))\n",
    "print(min(data_train['NN']))\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(data_train['NN'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'NN' is NA\n",
    "\n",
    "print(len(data_train))\n",
    "data_train = data_train.dropna(subset=['NN'])\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data_train[data_train['NN'] < 0.01]) + len(data_train[data_train['NN'] > 100]))\n",
    "\n",
    "# print(len(data_train))\n",
    "# data_train = data_train[data_train['NN'] >= 0.01]\n",
    "# print(len(data_train))\n",
    "# data_train = data_train[data_train['NN'] <= 100]\n",
    "# print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'NN' outliers\n",
    "\n",
    "print(len(data_train))\n",
    "\n",
    "z_scores = stats.zscore(data_train['NN'])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = abs_z_scores < 2\n",
    "\n",
    "data_train = data_train[filtered_entries]\n",
    "\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['NN'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['NN'].plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), title='NNs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(data_train['NN']))\n",
    "print(min(data_train['NN']))\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(data_train['NN'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['NN'].plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), title='NNs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['NN'].quantile(.05))\n",
    "print(data_train['NN'].quantile(.95))\n",
    "\n",
    "print(data_train['NN'].quantile(.10))\n",
    "print(data_train['NN'].quantile(.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NN_range_class(NN):    \n",
    "    class_label = \"\"\n",
    "    if(NN < 1):\n",
    "        class_label = \"class0\"\n",
    "    elif(NN > 68):\n",
    "        class_label = \"class24\"\n",
    "    else:\n",
    "        res = str(int(NN) // 3 + 1)\n",
    "        class_label = \"class\" + res\n",
    "    return class_label\n",
    "\n",
    "data_train['NN_range_class'] = data_train[\"NN\"].apply(calculate_NN_range_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# aa = range(-5,80)\n",
    "# for a in aa:\n",
    "#     print(calculate_NN_range_class(a), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "\n",
    "data_test = data_test.replace(np.nan, '', regex=True)\n",
    "\n",
    "print(\"Total:\", len(data_test[\"NN\"]))\n",
    "print(\"Empty NN cases:\", len(data_test[data_test[\"NN\"] == \"\"]))\n",
    "\n",
    "aa = data_test[data_test[\"NN\"] != \"\"]\n",
    "mean_NN = aa['NN'].mean()\n",
    "print(\"Mean:\", mean_NN)\n",
    "\n",
    "aa['NN'].plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_value(value):\n",
    "    if(value == \"\"):\n",
    "        return mean_NN\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['NN'] = data_test[\"NN\"].apply(convert_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "\n",
    "data_test['NN_range_class'] = data_test[\"NN\"].apply(calculate_NN_range_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_train.keys()))\n",
    "data_train = data_train.drop(columns=['A', 'D', 'F', 'H', 'I', 'J', 'M', 'NN', 'O'])\n",
    "print(len(data_train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "print(data_test.keys())\n",
    "print(len(data_test.keys()))\n",
    "data_test = data_test.drop(columns=['D', 'F', 'H', 'I', 'J', 'M', 'NN'])\n",
    "print(len(data_test.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data_train[data_train['P']==7])\n",
    "display(data_train[data_train['P']!=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_train[data_train['P']==7].info())\n",
    "data_train[data_train['P']!=1].info()\n",
    "data_train[data_train['P']==1].info()\n",
    "# data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each case except P==1 convert NAN occurences in K to empty string ''\n",
    "\n",
    "data_train.loc[data_train['P']!=1,'K'] = data_train.loc[data_train['P']!=1,'K'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_train[data_train['P']==7].info())\n",
    "data_train[data_train['P']!=1].info()\n",
    "data_train[data_train['P']==1].info()\n",
    "# data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "data_train.loc[(data_train['P']!=1) & (data_train['K']==\"\"),'K'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train[data_train['P']!=1].info())\n",
    "print(data_train[data_train['P']==1].info())\n",
    "\n",
    "data_train = data_train.dropna(subset=['L', 'K'])\n",
    "\n",
    "print(data_train[data_train['P']!=1].info())\n",
    "print(data_train[data_train['P']==1].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_text(text):\n",
    "    return text.replace(\"_\",\" \").replace(\"~\",\" \").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\\"\",\"\").replace(\",\",\" \").replace(\"*\",\" \").replace(\"/\",\" \").replace(\"\\\\\",\" \").replace(\":\",\"\").replace(\"&\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"E\"] = data_train[\"E\"].apply(edit_text)\n",
    "data_train[\"G\"] = data_train[\"G\"].apply(edit_text)\n",
    "data_train[\"K\"] = data_train[\"K\"].apply(edit_text)\n",
    "data_train[\"L\"] = data_train[\"L\"].apply(edit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "\n",
    "data_test[\"E\"] = data_test[\"E\"].apply(edit_text)\n",
    "data_test[\"G\"] = data_test[\"G\"].apply(edit_text)\n",
    "data_test[\"K\"] = data_test[\"K\"].apply(edit_text)\n",
    "data_test[\"L\"] = data_test[\"L\"].apply(edit_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['P'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['P'].hist(figsize=(12,8), bins=len(data_train['P'].unique()), grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_cleaned_1 = data_train[data_train['P'] != 1]\n",
    "\n",
    "data_train_cleaned_1['P'].hist(figsize=(12,8), bins=len(data_train['P'].unique())-1, grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance fix 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second csv result generated with this\n",
    "\n",
    "# l='P'\n",
    "\n",
    "# g = df.groupby(l, group_keys=False)\n",
    "# balanced_df = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()))).reset_index(drop=True)\n",
    "\n",
    "# print(balanced_df['P'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance fix 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third csv result generated with this\n",
    "\n",
    "# Average occurrences (rounded value) in all classes except \"class 1\"\n",
    "avg_items_except_c1 = int(data_train[data_train['P']!=1]['P'].value_counts().mean())\n",
    "\n",
    "# 'Imbalance fix 2' was real avg - 108\n",
    "# avg_items_except_c1 = 150 # 'Imbalance fix 3'\n",
    "# avg_items_except_c1 = 131 # 'Imbalance fix 4'\n",
    "# avg_items_except_c1 = 123 # 'Imbalance fix 5'\n",
    "avg_items_except_c1 = 165 # 'Imbalance fix 7'\n",
    "\n",
    "print(avg_items_except_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c1 = data_train[data_train['P']==1].sample(n=avg_items_except_c1, random_state=42)\n",
    "df_c4 = data_train[data_train['P']==4].sample(n=avg_items_except_c1, random_state=42)\n",
    "df_c3 = data_train[data_train['P']==3].sample(n=avg_items_except_c1, random_state=42)\n",
    "# df_c1 = data_train[data_train['P']==1].sample(n=avg_items_except_c1+15, random_state=42)\n",
    "# df_c4 = data_train[data_train['P']==4].sample(n=avg_items_except_c1+15, random_state=42)\n",
    "# df_c3 = data_train[data_train['P']==3].sample(n=avg_items_except_c1+15, random_state=42)\n",
    "\n",
    "df_with_less_rows = data_train[(data_train['P']==2) | (data_train['P']==5) | (data_train['P']==6) | (data_train['P']==7)]\n",
    "print(df_with_less_rows['P'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [df_with_less_rows]\n",
    "for class_index, group in df_with_less_rows.groupby('P'):\n",
    "    print(len(group))\n",
    "    lst.append(group.sample(avg_items_except_c1-len(group), replace=True))\n",
    "df_c2_c5_c6_c7 = pd.concat(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_c1, df_c4, df_c3, df_c2_c5_c6_c7]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['P'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = result\n",
    "data_train = data_train.reset_index()\n",
    "data_train.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create concatenated column of other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['all_fields'] = data_train['B'] + \" \" + data_train['C'] + \" \" + data_train['E'] + \" \" + data_train['G'] + \" \" + data_train['K'] + \" \" + data_train['L'] + \" \" + data_train['NN_range_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['all_fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['all_fields'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "data_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "data_test['all_fields'] = data_test['B'] + \" \" + data_test['C'] + \" \" + data_test['E'] + \" \" + data_test['G'] + \" \" + data_test['K'] + \" \" + data_test['L'] + \" \" + data_test['NN_range_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test.shape)\n",
    "print()\n",
    "print(data_test.info())\n",
    "print()\n",
    "display(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['splitted_sentence'] = [word_tokenize(sentence) for sentence in data_train['all_fields']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['splitted_sentence'][0])\n",
    "print()\n",
    "print(data_train['splitted_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "data_test['splitted_sentence'] = [word_tokenize(sentence) for sentence in data_test['all_fields']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "print(data_test['splitted_sentence'][0])\n",
    "print()\n",
    "print(data_test['splitted_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "\n",
    "# # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "# tag_map = defaultdict(lambda : wn.NOUN)\n",
    "# tag_map['J'] = wn.ADJ\n",
    "# tag_map['V'] = wn.VERB\n",
    "# tag_map['R'] = wn.ADV\n",
    "# for index, sentence in enumerate(data_train['splitted_sentence']):\n",
    "#     # Declaring Empty List to store the words that follow the rules for this step\n",
    "#     Final_words = []\n",
    "#     # Initializing WordNetLemmatizer()\n",
    "#     word_Lemmatized = WordNetLemmatizer()\n",
    "#     # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "#     for word, tag in pos_tag(sentence):\n",
    "#         # if word not in stopwords.words('english') and word.isalpha():\n",
    "#         word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "#         Final_words.append(word_Final)\n",
    "#     data_train.loc[index,'tokens_temp_lemmatisat'] = str(Final_words)\n",
    "\n",
    "#########################################\n",
    "\n",
    "data_train['tokens'] = data_train['splitted_sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cell ###############################\n",
    "data_test['tokens'] = data_test['splitted_sentence'].astype(str)\n",
    "display(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(data_train['tokens'],data_train['P'],test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "#     mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    # mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    mytokens = [ word for word in mytokens if word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2), min_df=1, max_df=1.0)\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2), min_df=1, max_df=1.0)\n",
    "\n",
    "vectorizer.fit(data_train['tokens'])\n",
    "\n",
    "Train_X_vectorized = vectorizer.transform(Train_X)\n",
    "Test_X_vectorized = vectorizer.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(Test_Y, pred_test):    \n",
    "\n",
    "    con_mat = confusion_matrix(Test_Y, pred_test)\n",
    "    con_mat = pd.DataFrame(con_mat, range(1,8), range(1,8))\n",
    "   \n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.set(font_scale=1.5) \n",
    "    sns.heatmap(con_mat, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap='Blues', cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression (solver='newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "lr_classifier.fit(Train_X_vectorized, Train_Y)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_lr1 = lr_classifier.predict(Test_X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, pred_lr1, average='macro') # 'macro' - calculate F1 for each label and find their unweighted mean\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "conf_matrix(Test_Y, pred_lr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression (solver='newton-cg', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(solver='newton-cg', class_weight='balanced')\n",
    "\n",
    "lr2.fit(Train_X_vectorized, Train_Y)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_lr2 = lr2.predict(Test_X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, pred_lr2, average='macro')\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "\n",
    "conf_matrix(Test_Y, pred_lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(Train_X_vectorized, Train_Y)\n",
    "prediction_NB = classifier.predict(Test_X_vectorized)\n",
    "print(\"Naive Bayes Accuracy Score: {}%\".format(round(accuracy_score(prediction_NB, Test_Y)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, prediction_NB, average='macro')\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "conf_matrix(Test_Y, prediction_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier\n",
    "svm_classifier = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "svm_classifier.fit(Train_X_vectorized,Train_Y)\n",
    "prediction_SVM = svm_classifier.predict(Test_X_vectorized)\n",
    "print(\"SVM Accuracy Score: {}%\".format(round(accuracy_score(prediction_SVM, Test_Y)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, prediction_SVM, average='macro')\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "conf_matrix(Test_Y, prediction_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (solver='lbfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression classifier\n",
    "classifier = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
    "classifier.fit(Train_X_vectorized,Train_Y)\n",
    "prediction_lr3 = classifier.predict(Test_X_vectorized)\n",
    "print(\"Logistic Regression Accuracy Score: {}%\".format(round(accuracy_score(prediction_lr3, Test_Y)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, prediction_lr3, average='macro')\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "conf_matrix(Test_Y, prediction_lr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (solver='lbfgs', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression classifier\n",
    "classifier = LogisticRegression(solver='lbfgs', max_iter=2000, class_weight='balanced')\n",
    "classifier.fit(Train_X_vectorized,Train_Y)\n",
    "prediction_lr4 = classifier.predict(Test_X_vectorized)\n",
    "print(\"Logistic Regression Accuracy Score: {}%\".format(round(accuracy_score(prediction_lr4, Test_Y)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, prediction_lr4, average='macro')\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "conf_matrix(Test_Y, prediction_lr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=510, random_state=42, verbose=1) # Add verbose=3 (more than 1) to see progress\n",
    "rf_classifier.fit(Train_X_vectorized,Train_Y)\n",
    "prediction_randomforest = rf_classifier.predict(Test_X_vectorized)\n",
    "print(\"Random Forest Accuracy Score: {}%\".format(round(accuracy_score(prediction_randomforest, Test_Y)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, prediction_randomforest, average='macro')\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "conf_matrix(Test_Y, prediction_randomforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "classifier = RandomForestClassifier(n_estimators=510, random_state=42, verbose=1, class_weight='balanced') # Add verbose=3 (more than 1) to see progress\n",
    "classifier.fit(Train_X_vectorized,Train_Y)\n",
    "prediction_randomforest2 = classifier.predict(Test_X_vectorized)\n",
    "print(\"Random Forest Accuracy Score: {}%\".format(round(accuracy_score(prediction_randomforest2, Test_Y)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(Test_Y, prediction_randomforest2, average='macro')\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "conf_matrix(Test_Y, prediction_randomforest2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes: {}%\".format(round(accuracy_score(prediction_NB, Test_Y)*100, 2)))\n",
    "print()\n",
    "print(\"SVM: {}%\".format(round(accuracy_score(prediction_SVM, Test_Y)*100, 2)))\n",
    "print(confusion_matrix(Test_Y, prediction_SVM))\n",
    "print()\n",
    "print(\"Logistic Regression: {}%\".format(round(accuracy_score(prediction_lr4, Test_Y)*100, 2)))\n",
    "print(\"Random Forest: {}%\".format(round(accuracy_score(prediction_randomforest, Test_Y)*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "gt_test_X_vectorized = vectorizer.transform(data_test['tokens'])\n",
    "\n",
    "print(gt_test_X_vectorized.shape)\n",
    "\n",
    "prediction_randomforest = rf_classifier.predict(gt_test_X_vectorized)\n",
    "\n",
    "print(prediction_randomforest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_test['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['P'] = pd.DataFrame(prediction_randomforest, columns = ['P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'res_7_/res_7.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4fun = pd.DataFrame(data_test['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4fun['P'] = np.random.randint(1, 7, df_4fun.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'res/res_3_df4fun.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
