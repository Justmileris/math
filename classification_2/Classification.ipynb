{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC8h8B1BUFzy"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG7FgtFxUrmv"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from chardet import detect\r\n",
        "import string\r\n",
        "\r\n",
        "from sklearn import model_selection, naive_bayes, svm\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from spacy.lang.en import English\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
        "\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "from collections import defaultdict\r\n",
        "from nltk.corpus import wordnet as wn\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk import pos_tag\r\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83YjsEemU1lG"
      },
      "source": [
        "%cd /content/gdrive/My Drive/EM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuDIPFQuU1tk"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU3EfmzXU1vw"
      },
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3STLQCoEVGwv"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wpunLH4U1x2"
      },
      "source": [
        "train_filepath = 'train.csv'\r\n",
        "test_filepath = 'test.csv'\r\n",
        "\r\n",
        "def get_encoding(file):\r\n",
        "    with open(file, 'rb') as f:\r\n",
        "        rawdata = f.read()\r\n",
        "    return detect(rawdata)['encoding']\r\n",
        "\r\n",
        "data_train = pd.read_csv(train_filepath, sep=\",\", encoding = get_encoding(train_filepath))\r\n",
        "data_test = pd.read_csv(test_filepath, sep=\",\", encoding = get_encoding(test_filepath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCbI1mfcU1zv"
      },
      "source": [
        "print(data_train.shape)\r\n",
        "print()\r\n",
        "print(data_train.columns)\r\n",
        "print()\r\n",
        "print(data_train.head())\r\n",
        "print()\r\n",
        "print(data_train.info())\r\n",
        "print()\r\n",
        "print(data_train.label_id.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQWQ8eb3VPT8"
      },
      "source": [
        "print(data_test.shape)\r\n",
        "print()\r\n",
        "print(data_test.columns)\r\n",
        "print()\r\n",
        "print(data_test.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7bYFvLLVV13"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlIEKyIHVPWH"
      },
      "source": [
        "data_train['Splitted_sentence'] = [word_tokenize(sentence) for sentence in data_train['name']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heqs8nBZVPYY"
      },
      "source": [
        "print(type(data_train))\r\n",
        "print(len(data_train))\r\n",
        "print(data_train['Splitted_sentence'][0])\r\n",
        "print(data_train['Splitted_sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6an9MR7WJLB"
      },
      "source": [
        "# Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2VDezXLVPah"
      },
      "source": [
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\r\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\r\n",
        "tag_map['J'] = wn.ADJ\r\n",
        "tag_map['V'] = wn.VERB\r\n",
        "tag_map['R'] = wn.ADV\r\n",
        "for index, sentence in enumerate(data_train['Splitted_sentence']):\r\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\r\n",
        "    Final_words = []\r\n",
        "    # Initializing WordNetLemmatizer()\r\n",
        "    word_Lemmatized = WordNetLemmatizer()\r\n",
        "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\r\n",
        "    for word, tag in pos_tag(sentence):\r\n",
        "        # if word not in stopwords.words('english') and word.isalpha():\r\n",
        "        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\r\n",
        "        Final_words.append(word_Final)\r\n",
        "    data_train.loc[index,'Tokens'] = str(Final_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sn-RwVTWpV3"
      },
      "source": [
        "# Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BghypBEVPcy"
      },
      "source": [
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(data_train['Tokens'],data_train['label_id'],test_size=0.3, random_state=42, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo4iJopxW5_f"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0T6yTrcWLcL"
      },
      "source": [
        "# Create our list of punctuation marks\r\n",
        "punctuations = string.punctuation\r\n",
        "\r\n",
        "# Create our list of stopwords\r\n",
        "nlp = spacy.load('en')\r\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\r\n",
        "\r\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\r\n",
        "parser = English()\r\n",
        "\r\n",
        "# Creating our tokenizer function\r\n",
        "def spacy_tokenizer(sentence):\r\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\r\n",
        "    mytokens = parser(sentence)\r\n",
        "\r\n",
        "    # Lemmatizing each token and converting each token into lowercase\r\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\r\n",
        "\r\n",
        "    # Removing stop words\r\n",
        "    # mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\r\n",
        "    mytokens = [ word for word in mytokens if word not in punctuations ]\r\n",
        "\r\n",
        "    # return preprocessed list of tokens\r\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPctMw-HWLeh"
      },
      "source": [
        "# vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2), min_df=1, max_df=1.0)\r\n",
        "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2), min_df=1, max_df=1.0)\r\n",
        "\r\n",
        "vectorizer.fit(data_train['Tokens'])\r\n",
        "\r\n",
        "Train_X_vectorized = vectorizer.transform(Train_X)\r\n",
        "Test_X_vectorized = vectorizer.transform(Test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRO1WgIKXZlb"
      },
      "source": [
        "print(len(vectorizer.vocabulary_))\r\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJX0gbXhXdbz"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooAAUzdUXd8q"
      },
      "source": [
        "# Naive Bayes classifier\r\n",
        "classifier = naive_bayes.MultinomialNB()\r\n",
        "classifier.fit(Train_X_vectorized, Train_Y)\r\n",
        "prediction_NB = classifier.predict(Test_X_vectorized)\r\n",
        "print(\"Naive Bayes Accuracy Score: {}%\".format(round(accuracy_score(prediction_NB, Test_Y)*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LoZFKDSXeP5"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzYkTfq7XfDH"
      },
      "source": [
        "# SVM classifier\r\n",
        "classifier = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\r\n",
        "classifier.fit(Train_X_vectorized,Train_Y)\r\n",
        "prediction_SVM = classifier.predict(Test_X_vectorized)\r\n",
        "print(\"SVM Accuracy Score: {}%\".format(round(accuracy_score(prediction_SVM, Test_Y)*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M3iatMkXfUm"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBGwhndNXfxE"
      },
      "source": [
        "# Logistic Regression classifier\r\n",
        "classifier = LogisticRegression(solver='lbfgs', max_iter=2000)\r\n",
        "classifier.fit(Train_X_vectorized,Train_Y)\r\n",
        "prediction_logreg = classifier.predict(Test_X_vectorized)\r\n",
        "print(\"Logistic Regression Accuracy Score: {}%\".format(round(accuracy_score(prediction_logreg, Test_Y)*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOxEQAceXf6X"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUsDePAwXgLW"
      },
      "source": [
        "# Random Forest\r\n",
        "classifier = RandomForestClassifier(n_estimators=10, random_state=42, verbose=3) # Add verbose=3 (more than 1) to see progress\r\n",
        "classifier.fit(Train_X_vectorized,Train_Y)\r\n",
        "prediction_randomforest = classifier.predict(Test_X_vectorized)\r\n",
        "print(\"Random Forest Accuracy Score: {}%\".format(round(accuracy_score(prediction_randomforest, Test_Y)*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixm8fwAtYSzC"
      },
      "source": [
        "# Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2inulOQXZng"
      },
      "source": [
        "print(\"Naive Bayes: {}%\".format(round(accuracy_score(prediction_NB, Test_Y)*100, 2)))\r\n",
        "print()\r\n",
        "print(\"SVM: {}%\".format(round(accuracy_score(prediction_SVM, Test_Y)*100, 2)))\r\n",
        "print(confusion_matrix(Test_Y, prediction_SVM))\r\n",
        "print()\r\n",
        "print(\"Logistic Regression: {}%\".format(round(accuracy_score(prediction_logreg, Test_Y)*100, 2)))\r\n",
        "print(\"Random Forest: {}%\".format(round(accuracy_score(prediction_randomforest, Test_Y)*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA7GC1xrYX4v"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaDhhjmOXZpj"
      },
      "source": [
        "ser = pd.Series(data=data_test['name'])\r\n",
        "\r\n",
        "gt_test_X_vectorized = vectorizer.transform(ser)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX1VN_2HYbI_"
      },
      "source": [
        "gt_test_X_vectorized.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdVjNijpYbK9"
      },
      "source": [
        "prediction_SVM = classifier.predict(gt_test_X_vectorized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPy8a8-bYbNL"
      },
      "source": [
        "print(prediction_SVM.shape)\r\n",
        "print(prediction_SVM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M69Xy5M4YbPn"
      },
      "source": [
        "df = pd.DataFrame(data_test['sku_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYElhmvSYbRX"
      },
      "source": [
        "df['label_id'] = pd.DataFrame(prediction_NB, columns = ['label_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiJyDh1FXZxG"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MhzeE9oYqQW"
      },
      "source": [
        "df.to_csv(r'res/res.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhXc7qw7YqSj"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mynmcGirYzJQ"
      },
      "source": [
        "# Random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G48jNQO3VPe9"
      },
      "source": [
        "df_4fun = pd.DataFrame(data_test['sku_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7fgzwm3Y1gX"
      },
      "source": [
        "df_4fun['label_id'] = np.random.randint(1, 7, df_4fun.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4JB-UwUY2_v"
      },
      "source": [
        "df_4fun"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A5Z1nXWY31P"
      },
      "source": [
        "df.to_csv(r'res/res_3_df4fun.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdserXjQY72H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}